---
title: "557_Project"
author: "Ben Straub"
date: "4/11/2017"
output: pdf_document
---

# Data overview  
 
Mining activity has long been associated with mining hazards, such as fires, floods, and toxic contaminants (Dozolme, P., 2016). Among these hazards, seismic hazards are the hardest to detect and predict (Sikora & Wróbel, 2010). Minimizing loss from seismic hazards requires both advanced data collection and analysis. In recent years, more and more advanced seismic and seismoacoustic monitoring systems have come about. Still, the disproportionate number of low-energy versus high-energy seismic phenomena (e.g. > $10^4$J) renders traditional analysis methods insufficient.

In this project, we used the seismic-bumps dataset provided by Sikora & Wróbel (2010), found in the UCI Machine Learning Repository. This seismic-bumps dataset comes from a coal mine located in Poland and contains 2584 observations of 19 attributes. Each observation summarizes seismic activity in the rock mass within one 8-hour shift. Note that the decision attribute, named "class", has values 1 and 0. This variable is the response variable we use in this project. A class value of "1" is categorized as "hazardous state", which essentially indicates a registered seismic bump with high energy (>$10^4$J) in the next shift. A class value "0" represents non-hazardous state in the next shift. According to Bukowska (2006), a number of factors having an effect on seismic hazard occurrence were proposed. Among other factors, the occurrence of tremors with energy > $10^4$J was listed. The purpose is to find whether and how the other 18 variables can be used to determine the hazard status of the mine.

### Table 1. Attribute information of the seismic-bumps dataset

 | Data Attributes | Description | Data Types | 
| -----------|-------------------------------------------------|----------|
| seismic   | result of shift seismic hazard assessment: 'a' - lack of hazard, 'b' - low hazard, 'c' - high hazard, 'd' - danger state  | Categorical   |
| seismoacoustic | result of shift seismic hazard assessment | Categorical  | 
| shift  | type of a shift: 'W' - coal-getting, 'N' - preparation shift  | Categorical  | 
| genergy  | seismic energy recorded within previous shift by active geophones (GMax) monitoring the longwall | Continuous  | 
| gpuls  | number of pulses recorded within previous shift by GMax | Continuous |
| gdenergy  | deviation of recorded energy within previous shift from average energy recorded during eight previous shifts  | Continuous  | 
| gdpuls | deviation of recorded pulses within previous shift from average number of pulses recorded during eight previous shifts  | Continuous  | 
| ghazard  | result of shift seismic hazard assessment by the seismoacoustic method based on registration coming from GMax  | Categorical | 
| nbumps   | the number of seismic bumps recorded within previous shift | Continuous  | 
| nbumps$i$, $i\in\{1,\ldots,5\}$  | the number of seismic bumps ($10^i-10^{i+1}$ J) registered within previous shift | Continuous  | 
| energy   | total energy of seismic bumps registered within previous shift  | Continuous  | 
| maxenergy  | maximum energy of the seismic bumps registered within previous shift  | Continuous  | 
| class  | the decision attribute: '1' - high energy seismic bump occurred in the next shift ('hazardous state'), '0' - no high energy seismic bumps occurred in th next shift ('non-hazardous state') | Categorical   | 


```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Configuring Space
#rm(list=ls())

# Loading packages into R
library(data.table);library(car);library(lars);library(knitr);library(ISLR);library(leaps);library(glmnet);library(MASS);library(reshape);library(ggplot2);library(pROC)
library(klaR);library(gridExtra)


#setwd("~/Box Sync/Skool/Spring 2017/557/Project-2-master")
#setwd("F:/Penn_State/Spring2017/STAT557/Workspace")
setwd("/Users/benStraub/Desktop/557/Project-3")
seismic <- read.csv("seismic.csv")
```


# Exploratory Data Analysis 
 
The state of the mine was indeed deemed hazardous infrequently $-$ only 170 shifts out of 2584 $-$ a difficult problem in our analyses. We want to examine which observations of seismic activity can help in the prediction of the hazard state of the mine during the next shift. Regression diagnostics indicate that the data, in general, meet most assumptions. However, we see that that data are somewhat skewed right, and there is severe multicollinearity (VIF > 10) between some of the covariates, as shown below.

\vspace{-14mm}

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height = 3}
##---------------------------------------------
## Some quick EDA from Hillary
##---------------------------------------------

par(mfrow=c(1,2))

seismic[,c(4:7,9:13,17:18)] <- seismic[,c(4:7,9:13,17:18)]
seismic <- seismic[,-(14:16)]

for(i in c(1:3,8)){
  seismic[,i] <- as.numeric(seismic[,i])
}

```

# Classification before Variable Selection 

We first take the seismic-bumps dataset and partition the data into training (75%) and test (25%) datasets. The next steps involve examining multiple classification methods on the training and test datasets separately. The goal is to examine which classification method outputs comparatively better prediction for seismic hazards based on available predictors.

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
##------------------------------------
## Setting up Test and Training Sets
##------------------------------------

# Divide into training and test
n <- dim(seismic)[1]
p <- dim(seismic)[2]

set.seed(2016)
test <- sample(n, round(n/4))
train <- (1:n)[-test]
seismic.train <- seismic[train,]
seismic.test <- seismic[test,]

```

# Logistic Regression

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA, fig.height = 2.5, fig.width = 3, fig.align='center'}

##--------------------------------------------
## Logistic Regression  Confusion\Roc
##--------------------------------------------
glm.train <- glm(class~., seismic.train, family=binomial)
#summary(glm.train)

glm.probs=predict(glm.train, type="response")
glm.pred=rep("0",1938)
glm.pred[glm.probs >.5]="1"

confusion1 <- table(glm.pred ,seismic.train$class)
#mean(glm.pred==seismic.train$class)

roc.Train <- roc(seismic.train$class, glm.probs, direction = "<")

sensitivity <- confusion1[2,2]/sum(confusion1[,2])
specificity <- confusion1[1,1]/sum(confusion1[,1])

#confusion1
#sensitivity
#specificity

glm.probs=predict(glm.train, seismic.test, type="response")

glm.pred=rep("0",646)
glm.pred[glm.probs >.5]="1"
confusion2 <- table(glm.pred, seismic.test$class)
#mean(glm.pred==seismic.test$class)

sensitivity <- confusion2[2,2]/sum(confusion2[,2])
specificity <- confusion2[1,1]/sum(confusion2[,1])

#confusion2
#sensitivity
#specificity

super <- cbind(confusion1,confusion2)
colnames(super) <- c("Train 0", "Train 1", "Test 0", "Test 1")
rownames(super) <- c("Predict 0", "Predict 1")
kable(super, caption="Training vs. Test for logistic regression")

roc.Test <- roc(seismic.test$class, glm.probs, direction="<")

par(mfrow = c(1,2))
plot.roc(roc.Test, col="blue", auc.polygon=TRUE,main="ROC Curve", xlab="False Positive Rate", ylab="True Positive Rate", print.auc=TRUE)
plot.roc(roc.Train, add=TRUE)

```

# Linear Discriminant Analysis

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}

lda.fit <- lda(class~., data = seismic, subset = train)

# TRAINING
lda.pred <- predict(lda.fit, seismic.train)
lda.class.train <- lda.pred$class

confusion <- table(lda.class.train,seismic.train$class)
sensitivity <- confusion[2,2]/sum(confusion[,2])
specificity <- confusion[1,1]/sum(confusion[,1])

confusion1 <- confusion

library(ROCR)
# choose the posterior probability column carefully, it may be 
# lda.pred$posterior[,1] or lda.pred$posterior[,2], depending on your factor levels 
pred <- prediction(lda.pred$posterior[,2], seismic.train$class) 
perf_train <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)

## Test
lda.pred.test <- predict(lda.fit, seismic.test)
lda.class.test <- lda.pred.test$class

confusion <- table(lda.class.test,seismic.test$class)
sensitivity <- confusion[2,2]/sum(confusion[,2])
specificity <- confusion[1,1]/sum(confusion[,1])

confusion2 <- confusion

pred <- prediction(lda.pred.test$posterior[,2], seismic.test$class) 
perf_test <- performance(pred,"tpr","fpr")
plot(perf_test,colorize=TRUE)
lines(perf_train)



```

# Random Forests Classifier 

```{r, echo=FALSE, warning=FALSE, message=FALSE, comment=NA}
library(randomForest)

## Random Forest for Classification Trees 

seismic$class = as.factor(seismic$class)

set.seed(2016)
test <- sample(n, round(n/4))
train <- (1:n)[-test]
seismic.train <- seismic[train,]
seismic.test <- seismic[test,]

## RF on Training Data 
start.time <- proc.time()

rf.seismic = randomForest(class~., data = seismic.train, importance = TRUE)
yhat.rf.train = predict(rf.seismic, newdata = seismic.train)
confusion <- table(yhat.rf.train, seismic.train$class)
sensitivity <- confusion[2,2]/sum(confusion[,2])
specificity <- confusion[1,1]/sum(confusion[,1])
#mean((yhat.rf.train - seismic.train$class)^2)
sensitivity
specificity
train.accuracy = (confusion[1,1]+confusion[2,2])/nrow(seismic.train)
train.accuracy

## RF on Test Data 

yhat.rf.test = predict(rf.seismic, newdata = seismic.test)
confusion <- table(yhat.rf.test, seismic.test$class)
sensitivity <- confusion[2,2]/sum(confusion[,2])
specificity <- confusion[1,1]/sum(confusion[,1])
#mean((yhat.rf.test - seismic.test$class)^2)
sensitivity
specificity
test.accuracy = (confusion[1,1]+confusion[2,2])/nrow(seismic.test)
test.accuracy


importance(rf.seismic)
varImpPlot(rf.seismic)

total.time <- proc.time() - start.time
time7 <- total.time[3] 
```

# Support vector classifier and support vector machine

```{r}
# Variable selection and refitting
#model1 = genergy + gpuls + nbumps + nbumps2 + nbumps4
#model2 = seismic + shift + gpuls + nbumps

library(e1071)

# We can modify this by using kernel = radial, which involves changing choice of gamma
# or by choosing kernel = polynomial, where we can also modify the degree
# Using a linear kernel is technically a support vector classifier

#--------------------------------------------------
# Get the ROC curve for svm
library(ROCR)

rocplot <- function(pred, truth, ...){
  predob <- prediction(pred, truth)
  perf <- performance(predob, "tpr", "fpr")
  plot(perf,...)
}
#--------------------------------------------------

#--------------------------------------------------
# Start with just the linear kernel
#--------------------------------------------------

##
## Model 1
##

start.time <- proc.time()

tune.out <- tune(svm, factor(class)~genergy + gpuls + nbumps + nbumps2 + nbumps4, data = seismic[train,], kernel = "linear", ranges = list(cost = c(.001,.01,.1,1,5)))

# Look for a best model
summary(tune.out)
bestmod <- tune.out$best.model
summary(bestmod)

ypred <- predict(bestmod, seismic[-train,])
table(predict = ypred, truth = seismic$class[-train])

svmfit.best1 <- svm(factor(class)~genergy + gpuls + nbumps + nbumps2 + nbumps4, data = seismic[train,], kernel = "linear", cost = bestmod$cost, decision.values = T)
fitted1 <- attributes(predict(svmfit.best1, seismic[train,], decision.values = T))$decision.values
fitted.test1 <- attributes(predict(svmfit.best1, seismic[-train,], decision.values = T))$decision.values

# It is unsurprising that this doesn't work well, because we are using a linear classifier
# However, we have reason to believe that a non-linear classifier would be more appropriate
rocplot(fitted1, seismic[train,"class"], main = "Training data")
rocplot(fitted.test1, seismic[-train,"class"], main = "Test data")

total.time <- proc.time() - start.time
time1 <- total.time[3]

##
## Model 2
##

start.time <- proc.time()

tune.out <- tune(svm, factor(class)~seismic + shift + gpuls + nbumps, data = seismic[train,], kernel = "linear", ranges = list(cost = c(.001,.01,.1,1,5)))

# Look for a best model
summary(tune.out)
bestmod <- tune.out$best.model
summary(bestmod)

ypred <- predict(bestmod, seismic[-train,])
table(predict = ypred, truth = seismic$class[-train])

svmfit.best2 <- svm(factor(class)~seismic + shift + gpuls + nbumps, data = seismic[train,], kernel = "linear", cost = bestmod$cost, decision.values = T)
fitted2 <- attributes(predict(svmfit.best2, seismic[train,], decision.values = T))$decision.values
fitted.test2 <- attributes(predict(svmfit.best2, seismic[-train,], decision.values = T))$decision.values

# This one shows a much better ROC curve
# But it still looks bad just from the original table produced
rocplot(fitted2, seismic[train,"class"], main = "Training data")
rocplot(fitted.test2, seismic[-train,"class"], main = "Test data")

total.time <- proc.time() - start.time
time2 <- total.time[3]

#--------------------------------------------------
# Implement with the radial kernel
#--------------------------------------------------

##
## Model 1
##

start.time <- proc.time()

tune.out2 <- tune(svm, factor(class)~genergy + gpuls + nbumps + nbumps2 + nbumps4, data = seismic[train,], kernel = "radial", ranges = list(cost = c(.001,.01,.1), gamma = c(1,5,50)))

bestmod <- tune.out2$best.model
ypred <- predict(bestmod, seismic[-train,])
table(predict = ypred, truth = seismic$class[-train])

svmrad2 <- svm(factor(class)~genergy + gpuls + nbumps + nbumps2 + nbumps4, data = seismic[train,], kernel = "radial", gamma = tune.out2$best.model$gamma, cost = tune.out2$best.model$cost, decision.values = T)
fitted2 <- attributes(predict(svmrad2, seismic[train,], decision.values = T))$decision.values
fitted.test2 <- attributes(predict(svmrad2, seismic[-train,],decision.values = T))$decision.values

rocplot(fitted2, seismic[train,"class"], main = "Training data")
rocplot(fitted.test2, seismic[-train,"class"], main = "Test data")

total.time <- proc.time() - start.time
time3 <- total.time[3]

##
## Model 2
##

start.time <- proc.time()

tune.out3 <- tune(svm, factor(class)~seismic + shift + gpuls + nbumps, data = seismic[train,], kernel = "radial", ranges = list(cost = c(.001,.01,.1), gamma = c(1,5,50)))

bestmod <- tune.out3$best.model
ypred <- predict(bestmod, seismic[-train,])
table(predict = ypred, truth = seismic$class[-train])

svmrad3 <- svm(factor(class)~seismic + shift + gpuls + nbumps, data = seismic[train,], kernel = "radial", gamma = tune.out3$best.model$gamma, cost = tune.out3$best.model$cost, decision.values = T)
fitted3 <- attributes(predict(svmrad3, seismic[train,], decision.values = T))$decision.values
fitted.test3 <- attributes(predict(svmrad3, seismic[-train,],decision.values = T))$decision.values

rocplot(fitted3, seismic[train,"class"], main = "Training data")
rocplot(fitted.test3, seismic[-train,"class"], main = "Test data")

total.time <- proc.time() - start.time
time4 <- total.time[3]

#--------------------------------------------------
# Implement with the polynomial kernel
#--------------------------------------------------

##
## Model 1
##

start.time <- proc.time()

tune.out4 <- tune(svm, factor(class)~genergy + gpuls + nbumps + nbumps2 + nbumps4, data = seismic[train,], kernel = "polynomial", ranges = list(cost = c(.001,.01,.1,), degree = c(2,3,4)))

bestmod <- tune.out4$best.model
ypred <- predict(bestmod, seismic[-train,])
table(predict = ypred, truth = seismic$class[-train])

svmpoly4 <- svm(factor(class)~genergy + gpuls + nbumps + nbumps2 + nbumps4, data = seismic[train,], kernel = "polynomial", cost = tune.out4$best.model$cost, degree = tune.out4$best.model$degree, decision.values = T)
fitted4 <- attributes(predict(svmpoly4, seismic[train,], decision.values = T))$decision.values
fitted.test4 <- attributes(predict(svmpoly4, seismic[-train,],decision.values = T))$decision.values

rocplot(fitted4, seismic[train,"class"], main = "Training data")
rocplot(fitted.test4, seismic[-train,"class"], main = "Test data")

total.time <- proc.time() - start.time
time5 <- total.time[3]

##
## Model 2
##

start.time <- proc.time()

tune.out5 <- tune(svm, factor(class)~seismic + shift + gpuls + nbumps, data = seismic[train,], kernel = "polynomial", ranges = list(cost = c(.001,.01,.1), degree = c(2,3,4)))

bestmod <- tune.out5$best.model
ypred <- predict(bestmod, seismic[-train,])
table(predict = ypred, truth = seismic$class[-train])

svmpoly5 <- svm(factor(class)~seismic + shift + gpuls + nbumps, data = seismic[train,], kernel = "polynomial", cost = tune.out5$best.model$cost, degree = tune.out5$best.model$degree, decision.values = T)
fitted5 <- attributes(predict(svmpoly5, seismic[train,], decision.values = T))$decision.values
fitted.test5 <- attributes(predict(svmpoly5, seismic[-train,],decision.values = T))$decision.values

rocplot(fitted5, seismic[train,"class"], main = "Training data")
rocplot(fitted.test5, seismic[-train,"class"], main = "Test data")

total.time <- proc.time() - start.time
time6 <- total.time[3]

```

# How to time your code!!!

```{r}
#-------------------------------------
# How to time your method
#-------------------------------------

# Put this before your method
start.time <- proc.time()

## the thing you are computing, like random forest or SVM goes here ##

total.time <- proc.time() - start.time

total.time[3] # the elapsed time

```
